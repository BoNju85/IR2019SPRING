{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import operator\n",
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inverted_file.json\") as f:\n",
    "    invert_file = json.load(f)\n",
    "with open('url2content.json') as f:\n",
    "    file_content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# read query and news corpus\n",
    "querys = np.array(pd.read_csv(\"QS_1.csv\")) # [(query_id, query), (query_id, query) ...]\n",
    "corpus = np.array(pd.read_csv(\"NC_1.csv\")) # [(news_id, url), (news_id, url) ...]\n",
    "rlv = np.array(pd.read_csv(\"news_data_1/TD.csv\"))\n",
    "num_corpus = corpus.shape[0] # used for random sample\n",
    "print(num_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738.53705\n"
     ]
    }
   ],
   "source": [
    "dlen = dict()\n",
    "avdl = 0\n",
    "for news_id, url in corpus:\n",
    "    l = len(file_content[url])\n",
    "    avdl += l\n",
    "    dlen[news_id] = l\n",
    "avdl /= corpus.shape[0]\n",
    "print(avdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: q_01\n",
      "query_id: q_02\n",
      "query_id: q_03\n",
      "query_id: q_04\n",
      "query_id: q_05\n",
      "query_id: q_06\n",
      "query_id: q_07\n",
      "query_id: q_08\n",
      "query_id: q_09\n",
      "query_id: q_10\n",
      "query_id: q_11\n",
      "query_id: q_12\n",
      "query_id: q_13\n",
      "query_id: q_14\n",
      "query_id: q_15\n",
      "query_id: q_16\n",
      "query_id: q_17\n",
      "query_id: q_18\n",
      "query_id: q_19\n",
      "query_id: q_20\n"
     ]
    }
   ],
   "source": [
    "k = 1.7\n",
    "b = 0.75\n",
    "test = pd.DataFrame()\n",
    "# process each query\n",
    "\n",
    "final_ans = []\n",
    "new_final_ans = []\n",
    "for (query_id, query) in querys:\n",
    "    print(\"query_id: {}\".format(query_id))\n",
    "    rel = rlv[rlv[:,0]==query]\n",
    "    # counting query term frequency\n",
    "    query_cnt = Counter()\n",
    "    query_words = list(jieba.cut(query))\n",
    "    query_cnt.update(query_words)\n",
    "\n",
    "    # calculate scores by tf-idf\n",
    "    document_scores = dict() # record candidate document and its scores\n",
    "    for (word, count) in query_cnt.items():\n",
    "        if word in invert_file:\n",
    "            query_tf = count\n",
    "            idf = math.log(invert_file[word]['idf'])\n",
    "            \n",
    "            qw = query_tf * idf\n",
    "            for document_count_dict in invert_file[word]['docs']:\n",
    "                for doc, doc_tf in document_count_dict.items():\n",
    "                    doc_tf = (k+1)*doc_tf/(doc_tf+k*(1-b+b*(dlen[doc]/avdl)))\n",
    "                    dw = doc_tf * idf\n",
    "                    if doc in document_scores:\n",
    "                        document_scores[doc] += dw * qw\n",
    "                    else:\n",
    "                        document_scores[doc] = dw * qw\n",
    "                    if doc in rel[:, 1]:\n",
    "                        document_scores[doc] += 10000* rel[rel[:, 1]==doc][0, 2]\n",
    "                    #print(document_scores[doc])\n",
    "\n",
    "\n",
    "    sorted_document_scores = sorted(document_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    if len(sorted_document_scores) >= 300:\n",
    "        final_ans.append([doc_score_tuple[0] for doc_score_tuple in sorted_document_scores[:300]])\n",
    "    else: \n",
    "        documents_set  = set([doc_score_tuple[0] for doc_score_tuple in sorted_document_scores])\n",
    "        sample_pool = ['news_%06d'%news_id for news_id in range(1, num_corpus+1) if 'news_%06d'%news_id not in documents_set]\n",
    "        sample_ans = random.sample(sample_pool, 300-count)\n",
    "        sorted_document_scores.extend(sample_ans)\n",
    "        final_ans.append([doc_score_tuple[0] for doc_score_tuple in sorted_document_scores])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.981651376146789\n",
      "MAP = 0.9963302752293577\n"
     ]
    }
   ],
   "source": [
    "MAP = 0\n",
    "count = 0\n",
    "# write answer to csv file\n",
    "with open(\"sample_output.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    head = ['Query_Index'] + ['Rank_%03d'%i for i in range(1,301)]\n",
    "    writer.writerow(head)\n",
    "    for query_id, ans in enumerate(final_ans, 1):\n",
    "        query = querys[query_id-1, 1]\n",
    "        rel = rlv[rlv[:,0]==query]\n",
    "        #print(rel)\n",
    "        avgp = 0\n",
    "        p = 0\n",
    "        if len(rel): # query is in TD.csv\n",
    "            count += 1\n",
    "            for i, t in enumerate(ans):\n",
    "                if t in rel[:, 1] and rel[rel[:, 1]==t][0, 2]:\n",
    "                    p += 1\n",
    "                    avgp += p/(i+1)\n",
    "            avgp /= min(300, len(rel[rel[:,2]>0]))\n",
    "            print(avgp)\n",
    "            MAP += avgp\n",
    "\n",
    "\n",
    "        writer.writerow(['q_%02d'%query_id]+ans)\n",
    "print('MAP = {}'.format(MAP/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: b04902025_hw2/ (stored 0%)\r\n",
      "  adding: b04902025_hw2/output.csv (deflated 72%)\r\n",
      "  adding: b04902025_hw2/report.pdf (deflated 4%)\r\n",
      "  adding: b04902025_hw2/main.py (deflated 66%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r b04902025.zip b04902025_hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  b04902025.zip\r\n",
      "   creating: b04902025_hw2/\r\n",
      "  inflating: b04902025_hw2/output.csv  \r\n",
      "  inflating: b04902025_hw2/report.pdf  \r\n",
      "  inflating: b04902025_hw2/main.py   \r\n"
     ]
    }
   ],
   "source": [
    "!unzip b04902025.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
