{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import jieba\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46972 cdn_loc_0001457\n"
     ]
    }
   ],
   "source": [
    "file_id = list()  # id to path\n",
    "with open('model/file-list') as f:\n",
    "    for line, doc in enumerate(f):\n",
    "        file_id.append(doc.split('/')[-1].strip().lower())\n",
    "file_num = len(file_id)\n",
    "print(len(file_id), file_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Doc Length: 862.501064463936\n"
     ]
    }
   ],
   "source": [
    "file_len = dict()  # path to len\n",
    "for root, dirs, files in os.walk('CIRB010/'):\n",
    "    for file in files:\n",
    "        path = root + '/' + file\n",
    "        with open(path) as f:\n",
    "            content = f.read()\n",
    "            content = re.sub(r'<\\/*[a-z]*>', '', content)\n",
    "            tmp = list(filter(None, content))\n",
    "            #print(len(tmp))\n",
    "            file_len[file.lower()] = len(content)\n",
    "avdl = sum(file_len.values())/file_num\n",
    "print('Average Doc Length: {}'.format(avdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copper\n"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "with open('model/vocab.all') as vfile:\n",
    "    vfile.readline()\n",
    "    for line, word in enumerate(vfile):\n",
    "        vocab[int(line)+1] = word.strip()\n",
    "\n",
    "print(vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 34.85046935081482\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "inverted = dict()  # word to count\n",
    "vid = 0\n",
    "with open('model/inverted-file') as f:\n",
    "    for line in f:\n",
    "        vid1, vid2, n = line.strip().split()\n",
    "        vid1 = int(vid1)\n",
    "        vid2 = int(vid2)\n",
    "        if vid2 != -1:\n",
    "            word = vocab[vid1]+vocab[vid2]\n",
    "        else:\n",
    "            word = vocab[vid1]\n",
    "        Count = dict()  # DocId to count of word\n",
    "        for i in range(int(n)):\n",
    "            DocID, count = f.readline().strip().split()\n",
    "            Count[int(DocID)] = int(count)\n",
    "            #print(DocID, freq[int(DocID)])\n",
    "        inverted[word] = Count\n",
    "        vid += 1\n",
    "print(\"Took {}\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans = pd.read_csv('queries/ans_train.csv')\n",
    "\n",
    "k = 1.2\n",
    "b = 0.75\n",
    "alpha = 0.8\n",
    "beta = 0.1\n",
    "gamma = 0.1\n",
    "upgrade_num = 3\n",
    "\n",
    "related = list()\n",
    "query_id = list()\n",
    "\n",
    "with open('queries/query-test.xml') as f:\n",
    "    content = f.read()\n",
    "    for qid, sub in enumerate(content.split('<topic>')[1:]):\n",
    "        sub = re.sub(r'\\n', '', sub)\n",
    "        n, t, q, nar, concepts = list(filter(None, re.split(r'<\\/*[a-z]*>', sub)))\n",
    "        concepts = re.split(r'、|。', concepts)[:-1]\n",
    "        \n",
    "        query_id.append(n[-3:])\n",
    "        \n",
    "        for concept in concepts:\n",
    "            if len(concept) > 2:\n",
    "                for i in range(len(concept)-1):\n",
    "                    concepts.append(concept[i:i+2])\n",
    "    \n",
    "        dv = np.zeros((file_num, len(concepts)))  # doc score\n",
    "        qv = np.zeros(len(concepts))\n",
    "        for i, word in enumerate(concepts):\n",
    "            if len(word) > 2:\n",
    "                continue\n",
    "            try:\n",
    "                df = sum(inverted[word].values())\n",
    "                IDF = math.log((file_num-df+0.5)/(df+0.5))\n",
    "                qv[i] += IDF\n",
    "                for occur, count in inverted[word].items():\n",
    "                    length = file_len[file_id[occur]]\n",
    "                    TF = (k+1)*count/(count+k*(1-b+b*length/avdl))\n",
    "                    #docw[occur][i] += TF/(1-b+b*length/avdl) * IDF\n",
    "                    dv[occur][i] += TF * IDF\n",
    "            except:\n",
    "                continue\n",
    "        #sim = np.sum(dv, 1)\n",
    "        sim = np.dot(dv, qv)\n",
    "        sim[np.isnan(sim)] = 0\n",
    "        \n",
    "        rid = [x for x in np.argsort(sim)[::-1][:100]]\n",
    "        irid = [x for x in np.argsort(sim)[::-1][100:200]]\n",
    "        topK = [file_id[x] for x in rid]\n",
    "        bottomK = [file_id[x] for x in irid]\n",
    "        \n",
    "        '''hit = 0\n",
    "        MAP = 0\n",
    "        ans = df_ans.loc[qid]['retrieved_docs'].strip().split()\n",
    "        for i, d in enumerate(topK):\n",
    "            if d in ans:\n",
    "                hit += 1\n",
    "                MAP += hit/(i+1)\n",
    "        print('MAP = {}'.format(MAP/(hit)))'''\n",
    "        \n",
    "        \n",
    "        for i in range(upgrade_num):\n",
    "            qv = alpha*qv + beta*np.sum([dv[x] for x in rid], 0)/100 - gamma*np.sum([dv[x] for x in irid], 0)/100\n",
    "            sim = np.dot(dv, qv)\n",
    "            sim[np.isnan(sim)] = 0\n",
    "\n",
    "            rid = [x for x in np.argsort(sim)[::-1][:100]]\n",
    "            irid = [x for x in np.argsort(sim)[::-1][100:200]]\n",
    "            topK = [file_id[x] for x in rid]\n",
    "            bottomK = [file_id[x] for x in irid]\n",
    "\n",
    "            '''hit = 0\n",
    "            MAP = 0\n",
    "            ans = df_ans.loc[qid]['retrieved_docs'].strip().split()\n",
    "            for j, d in enumerate(topK):\n",
    "                if d in ans:\n",
    "                    hit += 1\n",
    "                    MAP += hit/(j+1)\n",
    "            print('After upgrade {}times: MAP = {}'.format(i+1, MAP/(hit)))'''\n",
    "        \n",
    "        related.append(' '.join(topK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 31.4k/31.4k [00:09<00:00, 3.56kB/s]\n",
      "Successfully submitted to WM 2019 - VSM Model"
     ]
    }
   ],
   "source": [
    "df_ans = pd.DataFrame({'query_id': query_id, 'retrieved_docs': related})\n",
    "\n",
    "df_ans.to_csv('test.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c wm-2019-vsm-model -f test.csv -m \"doc length normalization, b=0.75\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
